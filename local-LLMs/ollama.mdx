---
title: "Setting up Ollama"
description: "Configure Ollama to work with Oryx for local AI capabilities"
icon: "robot"
---

## Quick Setup

<Steps>
  <Step title="Navigate to AI Settings">
    Navigate to `chrome://settings/oryx` to add Ollama as a provider.
  </Step>
  <Step title="Get Model ID">
    Get the model ID of your Ollama model (e.g., `gpt-oss:20b`)
  </Step>
  <Step title="Start Ollama Server">
    Start Ollama with CORS enabled:

    ```bash
    OLLAMA_ORIGINS="*" ollama serve
    ```

  </Step>
  <Step title="Select and Use">
    Select the model in agent and start using it! ðŸ¥³
  </Step>
</Steps>

<Warning>
  If you don't want to run from CLI with CORS settings, we recommend using LM
  Studio instead. See the [LM Studio setup guide](/local-LLMs/lm-studio).
</Warning>

## Alternative: LM Studio

<Card title="LM Studio Setup" icon="desktop" href="/local-LLMs/lm-studio">
  If you prefer not to run Ollama from the command line, LM Studio provides a
  more user-friendly alternative with a graphical interface.
</Card>
